{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Za-x6lscXMTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a43cf9c-f26a-4d89-f74f-33de968c2e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.11/dist-packages (0.3.6)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from bpemb) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bpemb) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bpemb) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from bpemb) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from bpemb) (4.67.1)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim->bpemb) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->bpemb) (7.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (2025.1.31)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->bpemb) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#%pip install tokenizers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install bpemb\n",
        "from bpemb import BPEmb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "88CUH-jiJjrj"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 32\n",
        "block_size = 100\n",
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI5jylsuEWWq"
      },
      "source": [
        "## Tokenizer Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "M4vh6qF_EWWr"
      },
      "outputs": [],
      "source": [
        "en_tokenizer = BPEmb(lang = 'en' , vs = 10000, dim = 50 , add_pad_emb=True)\n",
        "es_tokenizer = BPEmb(lang = 'es' , vs = 10000, dim = 50 , add_pad_emb=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xYgXSCmEWWr",
        "outputId": "b6f6d4c9-053e-45ec-94fe-edaccc618007"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‚ñÅhel',\n",
              " 'lo',\n",
              " '‚ñÅworld',\n",
              " ',',\n",
              " '‚ñÅthis',\n",
              " '‚ñÅis',\n",
              " '‚ñÅa',\n",
              " '‚ñÅreally',\n",
              " '‚ñÅlong',\n",
              " '‚ñÅsentence']"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "en_tokenizer.encode(\"Hello world, this is a really long sentence\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer.decode_ids('hel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-C-UWblbbEC",
        "outputId": "f0fed284-1ddc-408c-83cc-c7030ef0df14"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['h', 'e', 'l']"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer.decode([1, 908, 1418, 501, 9934, 215, 80, 4, 5920, 769, 8018, 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h953wOPmbA9x",
        "outputId": "6de0a4af-a71c-4f11-a04e-f9ac0148fe47"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'hel',\n",
              " 'lo',\n",
              " 'world',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'really',\n",
              " 'long',\n",
              " 'sentence',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "i5sDw1AlEWWr"
      },
      "outputs": [],
      "source": [
        "BOS_IDX = en_tokenizer.BOS\n",
        "EOS_IDX = en_tokenizer.EOS\n",
        "PAD_IDX = en_tokenizer.vs\n",
        "UNK_IDX = 0\n",
        "VOCAB_SIZE = en_tokenizer.vs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETcvWTIkEWWs"
      },
      "source": [
        "## Dataset Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "VoBwG3zuEWWs"
      },
      "outputs": [],
      "source": [
        "output_file = \"spa.txt\"\n",
        "with open(output_file, 'r' , encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "len(lines)\n",
        "lines = [line.split('\\t') for line in lines]\n",
        "lines = ['\\t'.join(line[:2]) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "iAgtOHr6EWWs"
      },
      "outputs": [],
      "source": [
        "train_lines, val_test_lines = train_test_split(lines, test_size=0.2, random_state=random_seed, shuffle=True)\n",
        "val_lines, test_lines = train_test_split(val_test_lines, test_size=0.5, random_state=random_seed, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIlN6YerEWWs",
        "outputId": "8939264a-2932-4259-db90-b9014bbc82da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113234\n",
            "14154\n",
            "14155\n"
          ]
        }
      ],
      "source": [
        "print(len(train_lines))\n",
        "print(len(val_lines))\n",
        "print(len(test_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eQb4no8KEWWt",
        "outputId": "92381d0d-7af2-4dee-e03e-f1f6e8a065ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't think I have one of those yet.\\tCreo que a√∫n no tengo uno de esos.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "train_lines[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "hLi8P105EWWt"
      },
      "outputs": [],
      "source": [
        "class SentencePairDataset(Dataset):\n",
        "    def __init__(self, lines,src_tokenizer , tgt_tokenizer):\n",
        "        super(SentencePairDataset, self).__init__()\n",
        "\n",
        "        self.lines = lines\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line = self.lines[idx]\n",
        "\n",
        "        src , tgt = line.split('\\t')\n",
        "        src_tokens = self.src_tokenizer.encode_ids_with_bos_eos(src)\n",
        "        tgt_tokens = self.tgt_tokenizer.encode_ids_with_bos_eos(tgt)\n",
        "\n",
        "        return src_tokens, tgt_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mttOqxBSEWWt"
      },
      "outputs": [],
      "source": [
        "train_ds = SentencePairDataset(train_lines,en_tokenizer,es_tokenizer)\n",
        "val_ds = SentencePairDataset(val_lines,en_tokenizer,es_tokenizer)\n",
        "test_ds = SentencePairDataset(test_lines,en_tokenizer,es_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Fhuk9KEWWt",
        "outputId": "e860b6f8-dae1-4eea-91a1-bfa1718bb2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Train Token Length Stats:\n",
            "   Max Length: 90\n",
            "   99th Percentile: 21.00\n",
            "   95th Percentile: 17.00\n",
            "   Median Length: 10.0\n",
            "   Mean Length: 10.68\n",
            "\n",
            "üìä Validation Token Length Stats:\n",
            "   Max Length: 53\n",
            "   99th Percentile: 21.00\n",
            "   95th Percentile: 17.00\n",
            "   Median Length: 10.0\n",
            "   Mean Length: 10.69\n",
            "\n",
            "üìä Test Token Length Stats:\n",
            "   Max Length: 39\n",
            "   99th Percentile: 21.00\n",
            "   95th Percentile: 16.00\n",
            "   Median Length: 10.0\n",
            "   Mean Length: 10.67\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_lengths = [len(x[0]) for x in train_ds]\n",
        "val_lengths = [len(x[0]) for x in val_ds]\n",
        "test_lengths = [len(x[0]) for x in test_ds]\n",
        "\n",
        "\n",
        "def compute_length_stats(lengths, name):\n",
        "    if not lengths:\n",
        "        print(f\"{name}: Dataset is empty!\")\n",
        "        return\n",
        "\n",
        "    lengths = np.array(lengths)\n",
        "    print(f\"üìä {name} Token Length Stats:\")\n",
        "    print(f\"   Max Length: {lengths.max()}\")\n",
        "    print(f\"   99th Percentile: {np.percentile(lengths, 99):.2f}\")\n",
        "    print(f\"   95th Percentile: {np.percentile(lengths, 95):.2f}\")\n",
        "    print(f\"   Median Length: {np.median(lengths)}\")\n",
        "    print(f\"   Mean Length: {lengths.mean():.2f}\\n\")\n",
        "\n",
        "# Compute stats for each dataset\n",
        "compute_length_stats(train_lengths, \"Train\")\n",
        "compute_length_stats(val_lengths, \"Validation\")\n",
        "compute_length_stats(test_lengths, \"Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afQ5QtpkEWWu"
      },
      "source": [
        "## Creating Dataset Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "grG9_IxcEWWu"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 100\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch_size = len(batch)\n",
        "    srcs, tgts = zip(*batch)\n",
        "    src_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "    tgt_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        src_vectors[i] = torch.tensor((srcs[i] + [PAD_IDX] * (max_seq_len - len(srcs[i])))[:max_seq_len], dtype=torch.long, device=device)\n",
        "        tgt_vectors[i] = torch.tensor((tgts[i] + [PAD_IDX] * (max_seq_len - len(tgts[i])))[:max_seq_len], dtype=torch.long, device=device)\n",
        "\n",
        "    return src_vectors, tgt_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hn6nAzJGEWWu"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn))\n",
        "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB0EFjhTEWWu",
        "outputId": "e5fb8f79-b5bd-47e6-ffd9-6bdd7deb60f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1,  2088,    80,  ..., 10000, 10000, 10000],\n",
            "        [    1,  1220,   597,  ..., 10000, 10000, 10000],\n",
            "        [    1,   386,  4599,  ..., 10000, 10000, 10000],\n",
            "        ...,\n",
            "        [    1,   326,  9937,  ..., 10000, 10000, 10000],\n",
            "        [    1,    83,  3048,  ..., 10000, 10000, 10000],\n",
            "        [    1,    83,  9937,  ..., 10000, 10000, 10000]], device='cuda:0')\n",
            "tensor([[    1,   772,   224,  ..., 10000, 10000, 10000],\n",
            "        [    1,   163,  3542,  ..., 10000, 10000, 10000],\n",
            "        [    1,   185,  4423,  ..., 10000, 10000, 10000],\n",
            "        ...,\n",
            "        [    1,  1292,   327,  ..., 10000, 10000, 10000],\n",
            "        [    1,   185,  4833,  ..., 10000, 10000, 10000],\n",
            "        [    1,   923,   520,  ..., 10000, 10000, 10000]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "for src , tgt in train_dataloader:\n",
        "    print(src)\n",
        "    print(tgt)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKT_b1DhEWWu"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-pGSFaq5Jjrp"
      },
      "outputs": [],
      "source": [
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "eval_iters = 200\n",
        "n_embd = 512\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.1\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qtzPYJaLEWWu"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n",
        "        self.n_embd = n_embd\n",
        "        self.n_head = n_head\n",
        "        self.d_k = n_embd // n_head\n",
        "\n",
        "        self.W_q = nn.Linear(n_embd, n_embd)\n",
        "        self.W_k = nn.Linear(n_embd, n_embd)\n",
        "        self.W_v = nn.Linear(n_embd, n_embd)\n",
        "        self.W_o = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def scaled_attention(self, Q, K, V, mask=None):\n",
        "        wei = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            wei = wei.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        wei = torch.softmax(wei, dim=-1)\n",
        "        output = torch.matmul(wei, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, n_embd = x.size()\n",
        "        return x.reshape(batch_size, seq_len, self.n_head, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).reshape(batch_size, seq_len, self.n_embd)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "k4KhEb0hEWWv"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Z6FVVI2NEWWv"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, n_embd, block_size):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(block_size, n_embd, device=device)\n",
        "        position = torch.arange(0, block_size, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.pow(10_000, (-torch.arange(0, n_embd, 2, device=device).float() / n_embd))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        return self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bp5EHUq9EWWv"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.feed_forward = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.ln1(x + self.dropout(self.self_attn(x, x, x, mask)))\n",
        "        x = self.ln2(x + self.feed_forward(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Jc0dIg2JEWWv"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.cross_attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.feed_forward = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ln3 = nn.LayerNorm(n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        x = self.ln1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n",
        "        x = self.ln2(x + self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask)))\n",
        "        x = self.ln3(x + self.feed_forward(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cyzLKmj3EWWv"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(VOCAB_SIZE + 1, n_embd , padding_idx= VOCAB_SIZE)\n",
        "        self.decoder_embedding = nn.Embedding(VOCAB_SIZE + 1, n_embd , padding_idx= VOCAB_SIZE)\n",
        "        self.positional_encoding = PositionalEncoding(n_embd, block_size)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
        "\n",
        "        self.fc = nn.Linear(n_embd, VOCAB_SIZE + 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zemitB-LEWWw",
        "outputId": "c5beeef5-4b13-4435-8e44-a764e7921fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59.510033 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = Transformer().to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWJvd6RJEWWw",
        "outputId": "bf03c202-3162-4660-eb69-8970fbe6c65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "-------\n",
            "Batch 0/1770 | Training Loss: 9.4260\n",
            "Batch 200/1770 | Training Loss: 4.6705\n",
            "Batch 400/1770 | Training Loss: 4.2067\n",
            "Batch 600/1770 | Training Loss: 3.6103\n",
            "Batch 800/1770 | Training Loss: 3.5125\n",
            "Batch 1000/1770 | Training Loss: 3.3236\n",
            "Batch 1200/1770 | Training Loss: 3.0222\n",
            "Batch 1400/1770 | Training Loss: 3.0409\n",
            "Batch 1600/1770 | Training Loss: 2.5775\n",
            "Epoch 1 Training Loss: 3.6201\n",
            "Validation Batch 0/222 | Validation Loss: 2.2980\n",
            "Validation Batch 50/222 | Validation Loss: 2.5430\n",
            "Validation Batch 100/222 | Validation Loss: 2.3318\n",
            "Validation Batch 150/222 | Validation Loss: 2.6572\n",
            "Validation Batch 200/222 | Validation Loss: 2.2523\n",
            "Epoch 1 Validation Loss: 2.4552\n",
            "Epoch 1 completed in 1059.65 seconds\n",
            "\n",
            "\n",
            "Epoch 2/3\n",
            "-------\n",
            "Batch 0/1770 | Training Loss: 2.3404\n",
            "Batch 200/1770 | Training Loss: 2.5484\n",
            "Batch 400/1770 | Training Loss: 2.1712\n",
            "Batch 600/1770 | Training Loss: 2.5689\n",
            "Batch 800/1770 | Training Loss: 2.3823\n",
            "Batch 1000/1770 | Training Loss: 2.0719\n",
            "Batch 1200/1770 | Training Loss: 2.0693\n",
            "Batch 1400/1770 | Training Loss: 1.7740\n",
            "Batch 1600/1770 | Training Loss: 1.9013\n",
            "Epoch 2 Training Loss: 2.1075\n",
            "Validation Batch 0/222 | Validation Loss: 1.8238\n",
            "Validation Batch 50/222 | Validation Loss: 1.7512\n",
            "Validation Batch 100/222 | Validation Loss: 1.6920\n",
            "Validation Batch 150/222 | Validation Loss: 1.4158\n",
            "Validation Batch 200/222 | Validation Loss: 1.9114\n",
            "Epoch 2 Validation Loss: 1.7680\n",
            "Epoch 2 completed in 1057.99 seconds\n",
            "\n",
            "\n",
            "Epoch 3/3\n",
            "-------\n",
            "Batch 0/1770 | Training Loss: 1.8877\n",
            "Batch 200/1770 | Training Loss: 1.5588\n",
            "Batch 400/1770 | Training Loss: 1.4934\n",
            "Batch 600/1770 | Training Loss: 1.5201\n",
            "Batch 800/1770 | Training Loss: 1.4382\n",
            "Batch 1000/1770 | Training Loss: 1.4692\n",
            "Batch 1200/1770 | Training Loss: 1.4612\n",
            "Batch 1400/1770 | Training Loss: 1.5522\n",
            "Batch 1600/1770 | Training Loss: 1.5175\n",
            "Epoch 3 Training Loss: 1.5526\n",
            "Validation Batch 0/222 | Validation Loss: 1.5689\n",
            "Validation Batch 50/222 | Validation Loss: 1.7160\n",
            "Validation Batch 100/222 | Validation Loss: 1.3283\n",
            "Validation Batch 150/222 | Validation Loss: 1.6025\n",
            "Validation Batch 200/222 | Validation Loss: 1.4194\n",
            "Epoch 3 Validation Loss: 1.5016\n",
            "Epoch 3 completed in 1057.06 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=VOCAB_SIZE)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "num_epochs = 3\n",
        "print_freq = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    num_train_batches = len(train_dataloader)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\\n\" + \"-------\")\n",
        "\n",
        "    for idx, (src_data, tgt_data) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, VOCAB_SIZE + 1), tgt_data[:, 1:].contiguous().view(-1))\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if idx % print_freq == 0:\n",
        "            print(f\"Batch {idx}/{num_train_batches} | Training Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_train_batches\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_val_batches = len(val_dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (src_data, tgt_data) in enumerate(val_dataloader):\n",
        "            output = model(src_data, tgt_data[:, :-1])\n",
        "            loss = criterion(output.contiguous().view(-1, VOCAB_SIZE + 1), tgt_data[:, 1:].contiguous().view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            if idx % 50 == 0:\n",
        "                print(f\"Validation Batch {idx}/{num_val_batches} | Validation Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_val_batches\n",
        "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/model/model_epoch_{epoch+1}.pth')\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1} completed in {elapsed_time:.2f} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uIEYwEzmEWWw",
        "outputId": "674b5a80-ddb7-4a00-9c8a-0ab7edd92342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.3822786808013916\n",
            "Test Loss: 1.4997037649154663\n",
            "Test Loss: 1.413204550743103\n",
            "Test Loss: 1.5136460065841675\n",
            "Test Loss: 1.3749380111694336\n",
            "Test Loss: 1.3223001956939697\n",
            "Test Loss: 1.4119950532913208\n",
            "Test Loss: 1.6161937713623047\n",
            "Test Loss: 1.5821583271026611\n",
            "Test Loss: 1.5933396816253662\n",
            "Test Loss: 1.3498424291610718\n",
            "Test Loss: 1.4229718446731567\n",
            "Test Loss: 1.6453138589859009\n",
            "Test Loss: 1.6780678033828735\n",
            "Test Loss: 1.5500739812850952\n",
            "Test Loss: 1.3732645511627197\n",
            "Test Loss: 1.3493176698684692\n",
            "Test Loss: 1.803459644317627\n",
            "Test Loss: 1.560603380203247\n",
            "Test Loss: 1.315798044204712\n",
            "Test Loss: 1.5363688468933105\n",
            "Test Loss: 1.453052282333374\n",
            "Test Loss: 1.8032400608062744\n",
            "Test Loss: 1.4793950319290161\n",
            "Test Loss: 1.4715509414672852\n",
            "Test Loss: 1.682931900024414\n",
            "Test Loss: 1.370986819267273\n",
            "Test Loss: 1.5008587837219238\n",
            "Test Loss: 1.4300965070724487\n",
            "Test Loss: 1.5177602767944336\n",
            "Test Loss: 1.4353270530700684\n",
            "Test Loss: 1.8620750904083252\n",
            "Test Loss: 1.3934544324874878\n",
            "Test Loss: 1.5801641941070557\n",
            "Test Loss: 1.4002538919448853\n",
            "Test Loss: 1.6470136642456055\n",
            "Test Loss: 1.7771308422088623\n",
            "Test Loss: 1.5375248193740845\n",
            "Test Loss: 1.340155005455017\n",
            "Test Loss: 1.3200470209121704\n",
            "Test Loss: 1.5709654092788696\n",
            "Test Loss: 1.5293233394622803\n",
            "Test Loss: 1.2493500709533691\n",
            "Test Loss: 1.4988694190979004\n",
            "Test Loss: 1.661705493927002\n",
            "Test Loss: 1.6477323770523071\n",
            "Test Loss: 1.4433073997497559\n",
            "Test Loss: 1.6003798246383667\n",
            "Test Loss: 1.403949499130249\n",
            "Test Loss: 1.4233207702636719\n",
            "Test Loss: 1.4500595331192017\n",
            "Test Loss: 1.619966983795166\n",
            "Test Loss: 1.510699987411499\n",
            "Test Loss: 1.627753734588623\n",
            "Test Loss: 1.7763662338256836\n",
            "Test Loss: 1.2994681596755981\n",
            "Test Loss: 1.6177129745483398\n",
            "Test Loss: 1.5517374277114868\n",
            "Test Loss: 1.5659679174423218\n",
            "Test Loss: 1.549638032913208\n",
            "Test Loss: 1.4334641695022583\n",
            "Test Loss: 1.5491483211517334\n",
            "Test Loss: 1.4074747562408447\n",
            "Test Loss: 1.3825676441192627\n",
            "Test Loss: 1.6650384664535522\n",
            "Test Loss: 1.8942290544509888\n",
            "Test Loss: 1.6168901920318604\n",
            "Test Loss: 1.4046822786331177\n",
            "Test Loss: 1.3396989107131958\n",
            "Test Loss: 1.4015060663223267\n",
            "Test Loss: 1.6816799640655518\n",
            "Test Loss: 1.6172267198562622\n",
            "Test Loss: 1.397017002105713\n",
            "Test Loss: 1.6787787675857544\n",
            "Test Loss: 1.4128164052963257\n",
            "Test Loss: 1.748605489730835\n",
            "Test Loss: 1.4245142936706543\n",
            "Test Loss: 1.6399258375167847\n",
            "Test Loss: 1.510291576385498\n",
            "Test Loss: 1.5842347145080566\n",
            "Test Loss: 1.5604232549667358\n",
            "Test Loss: 1.7300695180892944\n",
            "Test Loss: 1.4338923692703247\n",
            "Test Loss: 1.6615101099014282\n",
            "Test Loss: 1.3654966354370117\n",
            "Test Loss: 1.5640151500701904\n",
            "Test Loss: 1.3803796768188477\n",
            "Test Loss: 1.4183040857315063\n",
            "Test Loss: 1.246868371963501\n",
            "Test Loss: 1.6508458852767944\n",
            "Test Loss: 1.418904185295105\n",
            "Test Loss: 1.5047190189361572\n",
            "Test Loss: 1.6929748058319092\n",
            "Test Loss: 1.4546034336090088\n",
            "Test Loss: 1.5181186199188232\n",
            "Test Loss: 1.3733811378479004\n",
            "Test Loss: 1.5612232685089111\n",
            "Test Loss: 1.5450388193130493\n",
            "Test Loss: 1.7833881378173828\n",
            "Test Loss: 1.1906940937042236\n",
            "Test Loss: 1.4364299774169922\n",
            "Test Loss: 1.333794116973877\n",
            "Test Loss: 1.5214197635650635\n",
            "Test Loss: 1.3729850053787231\n",
            "Test Loss: 1.3735862970352173\n",
            "Test Loss: 1.476315975189209\n",
            "Test Loss: 1.6031080484390259\n",
            "Test Loss: 1.483810305595398\n",
            "Test Loss: 1.4819225072860718\n",
            "Test Loss: 1.6390174627304077\n",
            "Test Loss: 1.5157420635223389\n",
            "Test Loss: 1.7920676469802856\n",
            "Test Loss: 1.5186500549316406\n",
            "Test Loss: 1.4726710319519043\n",
            "Test Loss: 1.5039774179458618\n",
            "Test Loss: 1.5483707189559937\n",
            "Test Loss: 1.305202603340149\n",
            "Test Loss: 1.3591800928115845\n",
            "Test Loss: 1.811415195465088\n",
            "Test Loss: 1.475770115852356\n",
            "Test Loss: 1.6330697536468506\n",
            "Test Loss: 1.359944462776184\n",
            "Test Loss: 1.501423954963684\n",
            "Test Loss: 1.4132188558578491\n",
            "Test Loss: 1.7157182693481445\n",
            "Test Loss: 1.3902608156204224\n",
            "Test Loss: 1.7240275144577026\n",
            "Test Loss: 1.6590893268585205\n",
            "Test Loss: 1.3791495561599731\n",
            "Test Loss: 1.5418448448181152\n",
            "Test Loss: 1.6782536506652832\n",
            "Test Loss: 1.228214979171753\n",
            "Test Loss: 1.5508754253387451\n",
            "Test Loss: 1.404755711555481\n",
            "Test Loss: 1.423120379447937\n",
            "Test Loss: 1.4622383117675781\n",
            "Test Loss: 1.4826551675796509\n",
            "Test Loss: 1.2822823524475098\n",
            "Test Loss: 1.717645287513733\n",
            "Test Loss: 1.7976042032241821\n",
            "Test Loss: 1.4830886125564575\n",
            "Test Loss: 1.4873993396759033\n",
            "Test Loss: 1.5568957328796387\n",
            "Test Loss: 1.6328681707382202\n",
            "Test Loss: 1.679884672164917\n",
            "Test Loss: 1.5599085092544556\n",
            "Test Loss: 1.7009538412094116\n",
            "Test Loss: 1.5678026676177979\n",
            "Test Loss: 1.6311272382736206\n",
            "Test Loss: 1.6061843633651733\n",
            "Test Loss: 1.3653727769851685\n",
            "Test Loss: 1.7469919919967651\n",
            "Test Loss: 1.4107855558395386\n",
            "Test Loss: 1.08907151222229\n",
            "Test Loss: 1.4606575965881348\n",
            "Test Loss: 1.3803894519805908\n",
            "Test Loss: 1.7340153455734253\n",
            "Test Loss: 1.5491820573806763\n",
            "Test Loss: 1.3770103454589844\n",
            "Test Loss: 1.5974440574645996\n",
            "Test Loss: 1.2465879917144775\n",
            "Test Loss: 1.440759301185608\n",
            "Test Loss: 1.8139945268630981\n",
            "Test Loss: 1.4054642915725708\n",
            "Test Loss: 1.31001615524292\n",
            "Test Loss: 1.6078238487243652\n",
            "Test Loss: 1.5664446353912354\n",
            "Test Loss: 1.6760709285736084\n",
            "Test Loss: 1.4735701084136963\n",
            "Test Loss: 1.6234474182128906\n",
            "Test Loss: 1.4179103374481201\n",
            "Test Loss: 1.6061984300613403\n",
            "Test Loss: 1.662482738494873\n",
            "Test Loss: 1.4729293584823608\n",
            "Test Loss: 1.6912356615066528\n",
            "Test Loss: 1.5686906576156616\n",
            "Test Loss: 1.4519556760787964\n",
            "Test Loss: 1.205285906791687\n",
            "Test Loss: 1.2994813919067383\n",
            "Test Loss: 1.380589246749878\n",
            "Test Loss: 1.3648877143859863\n",
            "Test Loss: 1.481022596359253\n",
            "Test Loss: 1.5175780057907104\n",
            "Test Loss: 1.5223153829574585\n",
            "Test Loss: 1.5758788585662842\n",
            "Test Loss: 1.7265619039535522\n",
            "Test Loss: 1.3581246137619019\n",
            "Test Loss: 1.5231717824935913\n",
            "Test Loss: 1.4898526668548584\n",
            "Test Loss: 1.6162692308425903\n",
            "Test Loss: 1.4238780736923218\n",
            "Test Loss: 1.4294304847717285\n",
            "Test Loss: 1.5196105241775513\n",
            "Test Loss: 1.451067566871643\n",
            "Test Loss: 1.7836661338806152\n",
            "Test Loss: 1.6066855192184448\n",
            "Test Loss: 1.7140791416168213\n",
            "Test Loss: 1.559976577758789\n",
            "Test Loss: 1.5383046865463257\n",
            "Test Loss: 1.4849704504013062\n",
            "Test Loss: 1.6557241678237915\n",
            "Test Loss: 1.4640363454818726\n",
            "Test Loss: 1.8386478424072266\n",
            "Test Loss: 1.9396244287490845\n",
            "Test Loss: 1.6308300495147705\n",
            "Test Loss: 1.2738988399505615\n",
            "Test Loss: 1.2174543142318726\n",
            "Test Loss: 1.314708948135376\n",
            "Test Loss: 1.632674217224121\n",
            "Test Loss: 1.4454139471054077\n",
            "Test Loss: 1.2998192310333252\n",
            "Test Loss: 1.4441944360733032\n",
            "Test Loss: 1.7762877941131592\n",
            "Test Loss: 1.432665228843689\n",
            "Test Loss: 1.6790863275527954\n",
            "Test Loss: 1.651862621307373\n",
            "Test Loss: 1.5764511823654175\n",
            "Test Loss: 1.326326847076416\n",
            "Test Loss: 1.4941858053207397\n",
            "Test Loss: 1.3608793020248413\n",
            "Test Loss: 1.4725908041000366\n",
            "Test Loss: 1.728623628616333\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        src_data, tgt_data = data\n",
        "        output = model(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, VOCAB_SIZE + 1), tgt_data[:, 1:].contiguous().view(-1))\n",
        "        print(f\"Test Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gM-o8YkEWWw",
        "outputId": "cc2b55a4-3e5a-418c-d1f4-79b55d706e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-3cc2cbed6813>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/model/model_epoch_3.pth\"\n",
        "state_dict = torch.load(model_path)\n",
        "\n",
        "transformer_loaded = Transformer().to(device)\n",
        "transformer_loaded.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "G7eNN3OJEWWw"
      },
      "outputs": [],
      "source": [
        "def restore_word_boundaries(tokens):\n",
        "    \"\"\"Restores word boundaries from tokenized IDs and replaces `_` with spaces.\"\"\"\n",
        "    decoded_tokens = [es_tokenizer.emb.index_to_key[i] for i in tokens if i < es_tokenizer.vs]\n",
        "\n",
        "    # Replace \"_\" with space and properly join words\n",
        "    print(decoded_tokens)\n",
        "\n",
        "def translate(src):\n",
        "    src_tokens = en_tokenizer.encode_ids_with_bos_eos(src)\n",
        "    tgt_tokens = [BOS_IDX]\n",
        "\n",
        "    src_vectors = torch.tensor((src_tokens + [PAD_IDX] * (max_seq_len - len(src_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    for i in range(max_seq_len):\n",
        "        tgt_vectors = torch.tensor((tgt_tokens + [PAD_IDX] * (max_seq_len - len(tgt_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        output = transformer_loaded(src_vectors, tgt_vectors)\n",
        "        idx = torch.argmax(nn.functional.softmax(output, dim=2)[0][i]).item()\n",
        "        tgt_tokens.append(idx)\n",
        "\n",
        "        if idx == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return restore_word_boundaries(tgt_tokens[1 : -1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I am a teacher\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIEtGHS0am7y",
        "outputId": "b5ef081a-2c9b-42b8-ba39-430700756f22"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‚ñÅso', 'y', '‚ñÅprofesor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_tokenizer.decode_ids(195)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g5D58oOEiNgX",
        "outputId": "eb4b8308-f2cf-441d-90bd-8cca991107ab"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'so'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer.encode_ids('_')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTkCobykeOJS",
        "outputId": "31fe9223-0aa2-471a-f0a5-6f2531223725"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9912, 9976]"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_tokenizer.decode(es_tokenizer.encode(\"Hola Amigo\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XtbuVGEBUrYP",
        "outputId": "ea2d9023-d89c-4641-9f58-11d53f183c36"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hola amigo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_tokenizer.emb.index_to_key[195]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IxuUgsVDV-zF",
        "outputId": "eecbefa1-3ee9-4aa6-b2a3-8156602e6c29"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'‚ñÅso'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}